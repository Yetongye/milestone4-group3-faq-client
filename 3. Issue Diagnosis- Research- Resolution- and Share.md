# 3. Issue Diagnosis, Research, Resolution, and Sharing

## Issue #1: Database Migration – SQLite to PostgreSQL

### Description of the Issue

During deployment, the backend initially used SQLite as its database. While this worked locally, cloud platforms like Vercel and Railway do not support SQLite in production. The expected behavior was a functioning deployment, but the actual behavior was repeated database connection errors.

### Environment & Setup Details

- Local: Flask backend + SQLite
- Deployment: Vercel (frontend), Railway/Azure (backend)
- Database: Initially SQLite → required PostgreSQL for production

### Steps to Reproduce

- Run Flask with SQLite locally (works fine).
- Deploy Flask backend to Railway with SQLite config.
- Backend fails due to unsupported SQLite driver in cloud.

### Diagnosis

SQLite is file-based and not suited for production hosting. Railway and Azure require server-based relational databases (Postgres). The issue was a misalignment between development (SQLite) and production (Postgres).

### Research Process

- Reviewed Railway/Vercel deployment docs.
- Checked Flask-SQLAlchemy database connection guides.
- Consulted StackOverflow on SQLite vs PostgreSQL in cloud.
- Tested migration scripts from community blogs.

### Resolution Steps

- Wrote faq-backend/migrate\_sqlite\_to\_postgresql.py to migrate schema and data.
- Updated [config.py](http://config.py "config.py") and .env to include PostgreSQL connection string.
- Re-deployed backend on Railway with PostgreSQL database.
- Validated CRUD operations against PostgreSQL.

### Outcome Verification

- Verified database connectivity through Railway logs.
- Successfully executed CRUD operations (create, read, update, delete).
- Checked persisted data in PostgreSQL (pgAdmin).

### Knowledge Sharing

Documented migration steps and committed the script into the repository. Future maintainers can repeat this migration when switching databases.

## Issue #2: API Key Misconfiguration – OpenAI Integration Failure

### Description of the Issue  &#xA;

When testing the OpenAI GPT-3.5 integration, API requests failed with a 401 Unauthorized error. Expected behavior was successful response generation, but the backend returned authentication errors.

### Environment & Setup Details

- Backend: Flask running on Railway
- External Service: OpenAI GPT-3.5 API
- Configuration: Missing/incorrect API key in Railway environment variables

### Steps to Reproduce

1. Locally test Flask backend with .env containing OPENAI\_API\_KEY → works fine.
2. Deploy Flask backend to Railway.
3. Send requests from frontend → backend returns 401 error.

### Diagnosis  &#xA;

The root cause was missing or misconfigured environment variables on Railway. Locally, .env was loaded, but on Railway, the API key was not added to environment settings.

### Research Process

- Reviewed Railway documentation on environment variables.
- Searched for 401 Unauthorized OpenAI error on OpenAI docs and StackOverflow.
- Used AI tools to confirm correct Authorization: Bearer \<API\_KEY> format.

### Resolution Steps

- Logged into Railway project settings.
- Added environment variable OPENAI\_API\_KEY with correct value.
- Updated Flask backend to load the key from os.environ.
- Restarted backend service and retested integration.

### Outcome Verification

- Sent test prompt from frontend → received valid AI response.
- Logs confirmed successful API calls with 200 OK status.
- Monitored Railway dashboard for stable requests.

### Knowledge Sharing  &#xA;

Added deployment checklist: always configure secrets (DB URL, API keys) in Railway/Vercel before deploying. Documented API key management best practices for future contributors.

## Issue #3: CORS (Cross-Origin Resource Sharing) Error

### Description of the Issue

Frontend should be able to make API calls to the backend without any cross-origin restrictions. But when the frontend React application attempted to communicate with the Flask backend API, requests were being blocked by the browser with CORS policy errors.


**Actual behavior:** Browser console showed errors like:

```javascript 
Access to fetch at 'http://127.0.0.1:5000/api/chat' from origin 'http://localhost:3000' has been blocked by CORS policy: No 'Access-Control-Allow-Origin' header is present on the requested resource.
```


### Environment & Setup Details

- **Frontend:** React application running on `http://localhost:3000`
- **Backend:** Flask application running on `http://127.0.0.1:5000`
- **Browser:** Chrome 120.x
- **Development OS:** macOS
- **Flask version:** 2.3.x
- **React version:** 18.x

### Steps to Reproduce

1. Attempt to send a message through the chat interface
2. Observe the CORS error in browser console
3. API request fails and no response is received

### Diagnosis

The issue was caused by the browser's same-origin policy, which blocks requests from one domain (localhost:3000) to another domain (127.0.0.1:5000) unless the server explicitly allows cross-origin requests. The Flask backend was not configured to handle CORS headers, resulting in blocked requests.

### Research Process

**Resources consulted:**

1. **Stack Overflow:** Multiple threads about Flask CORS configuration
2. **Flask Documentation:** Official Flask documentation on handling cross-origin requests
3. **GitHub Issues:** Similar problems in Flask + React projects
4. **Trae Agent:** Ask Trae about the issue

**Key insights discovered:**

- CORS is a security feature implemented by browsers
- Flask requires explicit configuration to allow cross-origin requests
- Flask-CORS extension provides easy configuration options
- Different origins (localhost vs 127.0.0.1) are treated as separate domains

### Resolution Steps

1. **Install Flask-CORS extension:**
   ```bash 
   pip install flask-cors
   ```

2. **Import and configure CORS in **[**app.py**](http://app.py "app.py")**:**
   ```python 
   from flask_cors import CORS

   app = Flask(__name__)
   CORS(app, origins=['http://localhost:3000'])
   ```

3. **Alternative comprehensive configuration:**
   ```python 
   CORS(app, 
        origins=['http://localhost:3000', 'http://127.0.0.1:3000'],
        methods=['GET', 'POST', 'PUT', 'DELETE'],
        allow_headers=['Content-Type', 'Authorization'])
   ```

4. **Update requirements.txt:**
   ```python 
   flask-cors==4.0.0
   ```

5. **Restart the Flask development server**

### Outcome Verification

- **Frontend API calls:** Successfully completed without CORS errors
- **Network tab:** Requests showing proper CORS headers in response
- **Functionality test:** Chat interface working correctly with backend communication
- **Cross-browser testing:** Verified functionality in Chrome, Firefox, and Safari

## Issue #4: AI Service API Rate Limiting and Timeout

### Description of the Issue

The AI service integration (OpenAI API) was experiencing frequent timeouts and rate limiting errors, causing chat responses to fail or take excessively long to complete. But AI responses should be generated within 5-10 seconds with consistent availability.

**Actual behavior:** Users experienced:

- Request timeouts after 30+ seconds
- "Rate limit exceeded" errors during peak usage
- Inconsistent response times ranging from 3 seconds to 2 minutes
- Failed requests requiring manual retry

### Environment & Setup Details

- **AI Service:** OpenAI GPT-3.5-turbo API
- **API Client:** openai Python library 1.3.x
- **Request timeout:** Default (no explicit timeout set)
- **Rate limit tier:** Free tier (3 RPM, 40,000 TPM)
- **Concurrent users:** 5-10 simultaneous chat sessions
- **Average prompt length:** 200-500 tokens

### Steps to Reproduce

1. Start multiple chat sessions simultaneously
2. Send complex questions requiring detailed AI responses
3. Note timeout errors for longer responses

### Diagnosis

The issue was multifaceted:

1. **Rate limiting:** Free tier OpenAI API limits were insufficient for concurrent users
2. **No timeout configuration:** Requests could hang indefinitely
3. **No retry mechanism:** Failed requests were not automatically retried
4. **No caching:** Identical questions resulted in duplicate API calls
5. **No fallback strategy:** No graceful degradation when API was unavailable

### Research Process

**Resources consulted:**

1. **OpenAI Community Forum:** Common integration issues and solutions
2. **Python requests documentation:** Timeout and retry strategies
3. **Redis documentation:** Caching strategies for API responses
4. **GitHub repositories:** Similar AI integration projects

**Key insights discovered:**

- OpenAI free tier has strict rate limits (3 requests per minute)
- Proper timeout configuration is essential for user experience
- Caching can significantly reduce API calls for common questions
- Exponential backoff retry strategy improves reliability
- Background processing can improve perceived response times

### Resolution Steps

1. **Implement request timeout and retry logic in ai\_service.py:**
   ```python 
   def get_ai_response(question, max_retries=3):
       client = OpenAI(timeout=30.0)
       
       for attempt in range(max_retries):
           try:
               response = client.chat.completions.create(
                   model="gpt-3.5-turbo",
                   messages=[{"role": "user", "content": question}],
                   timeout=30
               )
               return response.choices[0].message.content
           except Exception as e:
               if attempt < max_retries - 1:
                   wait_time = (2 ** attempt) + random.uniform(0, 1)
                   time.sleep(wait_time)
               else:
                   raise e
   ```

2. **Add response caching with Redis:**
   ```python 
   redis_client = redis.Redis(host='localhost', port=6379, db=0)

   def get_cached_response(question):
       cache_key = hashlib.md5(question.encode()).hexdigest()
       cached = redis_client.get(cache_key)
       if cached:
           return json.loads(cached)
       return None

   def cache_response(question, response):
       cache_key = hashlib.md5(question.encode()).hexdigest()
       redis_client.setex(cache_key, 3600, json.dumps(response))  # 1 hour cache
   ```

3. **Implement fallback responses:**
   ```python 
   def get_ai_response_with_fallback(question):
       try:
           # Try cached response first
           cached = get_cached_response(question)
           if cached:
               return cached
           
           # Try AI service
           response = get_ai_response(question)
           cache_response(question, response)
           return response
           
       except Exception as e:
           logging.error(f"AI service error: {e}")
           return "I'm experiencing technical difficulties. Please try again later or contact support for assistance."
   ```

4. **Add rate limiting monitoring:**
   ```python 
   class RateLimiter:
       def __init__(self, max_requests=2, time_window=60):
           self.max_requests = max_requests
           self.time_window = time_window
           self.requests = deque()
       
       def can_make_request(self):
           now = time.time()
           # Remove old requests outside time window
           while self.requests and self.requests[0] < now - self.time_window:
               self.requests.popleft()
           
           return len(self.requests) < self.max_requests
       
       def record_request(self):
           self.requests.append(time.time())
   ```

5. **Add monitoring and alerting:**
   ```python 
   @app.route('/api/health/ai')
   def ai_health_check():
       try:
           test_response = get_ai_response("Hello", max_retries=1)
           return {"status": "healthy", "response_time": "< 5s"}
       except Exception as e:
           return {"status": "unhealthy", "error": str(e)}, 503
   ```


### Outcome Verification

- **Response time improvement:** Average response time reduced
- **Error rate reduction:** API errors decreased
- **User experience:** No more timeout errors reported by users
- **Rate limit compliance:** Successfully staying within API rate limits



